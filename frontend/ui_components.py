from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union, Protocol, Callable
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod
import io
import csv
import re
import logging
from contextlib import contextmanager

import pandas as pd
import streamlit as st
import numpy as np
from datetime import datetime

# Konfiguracja logowania
logger = logging.getLogger(__name__)

# ==============================
# KONFIGURACJA I TYPY
# ==============================

class DataSource(Enum):
    """≈πr√≥d≈Ça danych."""
    CSV = "csv"
    JSON = "json" 
    EXCEL = "excel"
    PARQUET = "parquet"
    DEMO = "demo"

class FileStatus(Enum):
    """Status przetwarzania pliku."""
    SUCCESS = "success"
    WARNING = "warning"
    ERROR = "error"
    TOO_LARGE = "too_large"
    INVALID_FORMAT = "invalid_format"
    ENCODING_ERROR = "encoding_error"
    EMPTY_FILE = "empty_file"

@dataclass
class FileMetadata:
    """Metadane wgranego pliku."""
    filename: str
    size_mb: float
    format: str
    rows: int = 0
    columns: int = 0
    status: FileStatus = FileStatus.SUCCESS
    warnings: List[str] = field(default_factory=list)
    encoding: Optional[str] = None
    separator: Optional[str] = None

@dataclass
class DataConfig:
    """Konfiguracja wczytywania danych."""
    max_file_size_mb: int = 200
    max_preview_rows: int = 20
    supported_formats: List[str] = field(default_factory=lambda: ["csv", "json", "xlsx", "parquet"])
    auto_detect_encoding: bool = True
    auto_detect_separator: bool = True

@dataclass 
class UIConfig:
    """Konfiguracja interfejsu u≈ºytkownika."""
    app_title: str = "TMIV ‚Äî The Most Important Variables"
    app_subtitle: str = "AutoML ‚Ä¢ EDA ‚Ä¢ Historia eksperyment√≥w"
    show_advanced_options: bool = True
    enable_llm: bool = True
    enable_demo_data: bool = True
    demo_data_paths: List[Path] = field(default_factory=lambda: [
        Path("data/avocado.csv"),
        Path("datasets/avocado.csv"), 
        Path("avocado.csv")
    ])

# ==============================
# PROTOKO≈ÅY (INTERFACES)
# ==============================

class FileFormatDetector(Protocol):
    """Interface dla detektor√≥w format√≥w plik√≥w."""
    def detect_encoding(self, data: bytes) -> str: ...
    def detect_delimiter(self, text: str) -> str: ...
    def detect_format(self, filename: str) -> str: ...

class FileValidator(Protocol):
    """Interface dla walidator√≥w plik√≥w."""
    def validate_size(self, size: int) -> bool: ...
    def validate_format(self, filename: str) -> bool: ...
    def validate_content(self, df: pd.DataFrame) -> List[str]: ...

class DataLoader(Protocol):
    """Interface dla loader√≥w danych."""
    def load_data(self, file_data: bytes, **kwargs) -> pd.DataFrame: ...

# ==============================
# IMPLEMENTACJE DETEKTOR√ìW
# ==============================

class SmartFileFormatDetector:
    """Inteligentny detektor format√≥w i parametr√≥w plik√≥w."""
    
    def detect_encoding(self, sample: bytes, max_size: int = 64 * 1024) -> str:
        """Wykrywa encoding z fallbackiem."""
        sample = sample[:max_size]
        encodings = ["utf-8", "cp1250", "latin1", "utf-16", "ascii"]
        
        for encoding in encodings:
            try:
                decoded = sample.decode(encoding)
                # Sprawd≈∫ czy nie ma dziwnych znak√≥w
                if not any(ord(c) > 127 and ord(c) < 160 for c in decoded[:1000]):
                    return encoding
            except (UnicodeDecodeError, UnicodeError):
                continue
        
        return "utf-8"  # Fallback
    
    def detect_delimiter(self, sample: str, max_lines: int = 50) -> str:
        """Wykrywa separator CSV z analizƒÖ sp√≥jno≈õci."""
        lines = sample.split('\n')[:max_lines]
        
        try:
            # U≈ºyj CSV Sniffer
            dialect = csv.Sniffer().sniff(sample[:8192], delimiters=",;\t|")
            return dialect.delimiter
        except Exception:
            pass
        
        # Fallback: analiza sp√≥jno≈õci
        delimiters = [",", ";", "\t", "|"]
        scores = {}
        
        for delim in delimiters:
            counts = [line.count(delim) for line in lines if line.strip()]
            if not counts:
                continue
                
            most_common_count = max(set(counts), key=counts.count) if counts else 0
            consistency = counts.count(most_common_count) / len(counts) if counts else 0
            scores[delim] = (most_common_count, consistency)
        
        if not scores:
            return ","
        
        # Wybierz delimiter z najlepszƒÖ sp√≥jno≈õciƒÖ
        best_delim = max(scores.items(), key=lambda x: (x[1][1], min(x[1][0], 20)))
        return best_delim[0]
    
    def detect_format(self, filename: str) -> str:
        """Wykrywa format pliku na podstawie rozszerzenia."""
        suffix = Path(filename).suffix.lower().lstrip('.')
        format_mapping = {
            'csv': 'csv',
            'txt': 'csv', 
            'json': 'json',
            'jsonl': 'json',
            'xlsx': 'excel',
            'xls': 'excel',
            'parquet': 'parquet'
        }
        return format_mapping.get(suffix, 'csv')

class BasicFileValidator:
    """Podstawowy walidator plik√≥w."""
    
    def __init__(self, config: DataConfig):
        self.config = config
    
    def validate_size(self, size_bytes: int) -> bool:
        """Waliduje rozmiar pliku."""
        size_mb = size_bytes / (1024 * 1024)
        return size_mb <= self.config.max_file_size_mb
    
    def validate_format(self, filename: str) -> bool:
        """Waliduje format pliku."""
        format_name = SmartFileFormatDetector().detect_format(filename)
        return format_name in self.config.supported_formats
    
    def validate_content(self, df: pd.DataFrame) -> List[str]:
        """Waliduje zawarto≈õƒá DataFrame."""
        warnings = []
        
        if df.empty:
            warnings.append("Plik jest pusty")
        
        if len(df.columns) == 0:
            warnings.append("Brak kolumn w danych")
        
        if len(df.columns) > 1000:
            warnings.append("Bardzo du≈ºo kolumn (>1000) - mo≈ºe wp≈ÇynƒÖƒá na wydajno≈õƒá")
        
        return warnings

# ==============================
# LOADERY DANYCH
# ==============================

class CSVDataLoader:
    """Loader dla plik√≥w CSV."""
    
    def __init__(self, detector: FileFormatDetector):
        self.detector = detector
    
    def load_data(self, file_data: bytes, **kwargs) -> pd.DataFrame:
        """≈Åaduje plik CSV z auto-detekcjƒÖ lub override parametr√≥w."""
        
        # Auto-detekcja lub u≈ºycie podanych parametr√≥w
        encoding = kwargs.get('encoding') or self.detector.detect_encoding(file_data)
        
        try:
            decoded = file_data.decode(encoding, errors='ignore')
        except Exception:
            decoded = file_data.decode('utf-8', errors='ignore')
            encoding = 'utf-8'
        
        delimiter = kwargs.get('delimiter') or self.detector.detect_delimiter(decoded)
        decimal = kwargs.get('decimal', '.')
        header = kwargs.get('header', 0)
        
        # Wczytaj DataFrame
        df = pd.read_csv(
            io.BytesIO(file_data),
            encoding=encoding,
            sep=delimiter,
            decimal=decimal,
            header=header,
            low_memory=False,
            na_values=['', 'NULL', 'null', 'N/A', 'n/a', '#N/A'],
            keep_default_na=True
        )
        
        # Je≈õli brak header, nadaj nazwy kolumn
        if header is None:
            df.columns = [f"col_{i}" for i in range(len(df.columns))]
        
        return df

class JSONDataLoader:
    """Loader dla plik√≥w JSON."""
    
    def load_data(self, file_data: bytes, **kwargs) -> pd.DataFrame:
        """≈Åaduje pliki JSON z obs≈ÇugƒÖ r√≥≈ºnych format√≥w."""
        
        decoded = file_data.decode('utf-8', errors='ignore')
        
        # Pr√≥buj r√≥≈ºne formaty JSON
        formats_to_try = [
            ('records', lambda: pd.read_json(io.StringIO(decoded), orient='records')),
            ('lines', lambda: pd.read_json(io.StringIO(decoded), lines=True)),
            ('auto', lambda: pd.read_json(io.StringIO(decoded))),
        ]
        
        for format_name, loader in formats_to_try:
            try:
                df = loader()
                if df is not None and not df.empty:
                    return df
            except Exception:
                continue
        
        raise ValueError("Nie uda≈Ço siƒô wczytaƒá danych z pliku JSON")

class ExcelDataLoader:
    """Loader dla plik√≥w Excel."""
    
    def load_data(self, file_data: bytes, **kwargs) -> pd.DataFrame:
        """≈Åaduje pliki Excel."""
        sheet_name = kwargs.get('sheet_name', 0)
        header = kwargs.get('header', 0)
        
        df = pd.read_excel(
            io.BytesIO(file_data),
            sheet_name=sheet_name,
            header=header
        )
        
        return df

# ==============================
# FACTORY DLA LOADER√ìW
# ==============================

class DataLoaderFactory:
    """Factory do tworzenia odpowiednich loader√≥w danych."""
    
    def __init__(self):
        self.detector = SmartFileFormatDetector()
        self.loaders = {
            'csv': CSVDataLoader(self.detector),
            'json': JSONDataLoader(),
            'excel': ExcelDataLoader()
        }
    
    def get_loader(self, format_name: str) -> DataLoader:
        """Zwraca odpowiedni loader dla formatu."""
        return self.loaders.get(format_name, self.loaders['csv'])

# ==============================
# G≈Å√ìWNE KOMPONENTY UI
# ==============================

class FileUploadWidget:
    """Uproszczony widget do uploadu plik√≥w."""
    
    def __init__(self, config: DataConfig):
        self.config = config
        self.detector = SmartFileFormatDetector()
        self.validator = BasicFileValidator(config)
        self.loader_factory = DataLoaderFactory()
    
    def render(self, key: str = "file_uploader") -> Tuple[Optional[pd.DataFrame], Optional[FileMetadata]]:
        """Renderuje widget upload z podstawowymi opcjami."""
        
        uploaded_file = st.file_uploader(
            "Wgraj plik danych",
            type=self.config.supported_formats,
            key=key,
            help=f"Maksymalny rozmiar: {self.config.max_file_size_mb}MB"
        )
        
        if not uploaded_file:
            return None, None
        
        # Walidacja podstawowa
        if not self.validator.validate_size(uploaded_file.size):
            st.error(f"Plik za du≈ºy! ({uploaded_file.size / 1024 / 1024:.1f}MB > {self.config.max_file_size_mb}MB)")
            return None, None
        
        if not self.validator.validate_format(uploaded_file.name):
            st.error(f"Nieobs≈Çugiwany format pliku")
            return None, None
        
        # Wczytywanie danych
        try:
            format_name = self.detector.detect_format(uploaded_file.name)
            loader = self.loader_factory.get_loader(format_name)
            
            file_data = uploaded_file.read()
            uploaded_file.seek(0)
            
            df = loader.load_data(file_data)
            
            # Walidacja zawarto≈õci
            content_warnings = self.validator.validate_content(df)
            
            # Metadane
            metadata = FileMetadata(
                filename=uploaded_file.name,
                size_mb=uploaded_file.size / 1024 / 1024,
                format=format_name,
                rows=len(df),
                columns=len(df.columns),
                status=FileStatus.WARNING if content_warnings else FileStatus.SUCCESS,
                warnings=content_warnings
            )
            
            # Wy≈õwietl informacje o pliku
            self._display_file_info(df, metadata)
            
            return df, metadata
            
        except Exception as e:
            st.error(f"B≈ÇƒÖd wczytywania pliku: {str(e)}")
            return None, None
    
    def _display_file_info(self, df: pd.DataFrame, metadata: FileMetadata):
        """Wy≈õwietla informacje o wczytanym pliku."""
        
        # Status badge
        if metadata.status == FileStatus.SUCCESS:
            st.success(f"‚úÖ **{metadata.filename}** wczytany pomy≈õlnie")
        elif metadata.status == FileStatus.WARNING:
            st.warning(f"‚ö†Ô∏è **{metadata.filename}** wczytany z ostrze≈ºeniami")
        
        # Podstawowe metryki
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Wiersze", f"{metadata.rows:,}")
        with col2:
            st.metric("Kolumny", metadata.columns)
        with col3:
            st.metric("Rozmiar", f"{metadata.size_mb:.1f} MB")
        with col4:
            st.metric("Format", metadata.format.upper())
        
        # Ostrze≈ºenia
        if metadata.warnings:
            with st.expander("‚ö†Ô∏è Ostrze≈ºenia", expanded=False):
                for warning in metadata.warnings:
                    st.warning(warning)
        
        # PodglƒÖd danych
        preview_rows = min(self.config.max_preview_rows, len(df))
        with st.expander("üìä PodglƒÖd danych", expanded=True):
            st.dataframe(df.head(preview_rows), use_container_width=True)

class DemoDataWidget:
    """Widget do wyboru danych demonstracyjnych."""
    
    def __init__(self, demo_paths: List[Path]):
        self.demo_paths = demo_paths
    
    def render(self) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
        """Renderuje opcjƒô demo data."""
        
        st.info("üìä U≈ºywasz danych demonstracyjnych 'avocado'")
        
        try:
            for path in self.demo_paths:
                if path.exists():
                    df = pd.read_csv(path)
                    
                    # Podstawowe informacje
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Wiersze", f"{len(df):,}")
                    with col2:
                        st.metric("Kolumny", len(df.columns))
                    with col3:
                        st.metric("≈πr√≥d≈Ço", "Demo")
                    
                    # PodglƒÖd
                    with st.expander("üìä PodglƒÖd danych", expanded=True):
                        st.dataframe(df.head(10), use_container_width=True)
                    
                    return df, "avocado (demo)"
            
            raise FileNotFoundError("Nie znaleziono pliku demonstracyjnego")
            
        except Exception as e:
            st.error(f"B≈ÇƒÖd ≈Çadowania danych demo: {str(e)}")
            return None, None

class TargetSelectorWidget:
    """Uproszczony widget do wyboru kolumny celu."""
    
    def __init__(self):
        self.target_keywords = [
            "target", "y", "label", "class", "price", "amount", "value", 
            "revenue", "sales", "outcome", "result", "prediction", "AveragePrice"
        ]
    
    def render(self, columns: List[str]) -> Optional[str]:
        """Renderuje selector kolumny celu z heurystykƒÖ."""
        
        if not columns:
            st.warning("Nie mo≈ºna ustaliƒá kolumn ‚Äî wczytaj dane.")
            return None
        
        # Heurystyka znajdowania targetu
        auto_target = self._detect_target(columns)
        
        # Selector z podpowiedziƒÖ
        default_index = columns.index(auto_target) if auto_target in columns else 0
        
        selected_target = st.selectbox(
            "üéØ Wybierz kolumnƒô celu",
            options=columns,
            index=default_index,
            help="Kolumna, kt√≥rƒÖ chcesz przewidywaƒá"
        )
        
        if selected_target != auto_target and auto_target:
            st.info(f"üí° Podpowied≈∫: wykryto potencjalny target '{auto_target}'")
        
        return selected_target
    
    def _detect_target(self, columns: List[str]) -> Optional[str]:
        """Heurystyka znajdowania kolumny celu."""
        column_lower = {c.lower(): c for c in columns}
        
        for keyword in self.target_keywords:
            if keyword.lower() in column_lower:
                return column_lower[keyword.lower()]
        
        return None

class DataSourceSelectorWidget:
    """Widget do wyboru ≈∫r√≥d≈Ça danych."""
    
    def __init__(self, config: DataConfig, ui_config: UIConfig):
        self.config = config
        self.ui_config = ui_config
        self.file_widget = FileUploadWidget(config)
        self.demo_widget = DemoDataWidget(ui_config.demo_data_paths) if ui_config.enable_demo_data else None
    
    def render(self) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
        """Renderuje selector ≈∫r√≥d≈Ça danych."""
        
        # Opcje ≈∫r√≥de≈Ç
        options = ["Wgraj w≈Çasny plik"]
        if self.ui_config.enable_demo_data:
            options.append("U≈ºyj danych demo 'avocado'")
        
        source_type = st.radio(
            "üì¶ ≈πr√≥d≈Ço danych", 
            options, 
            horizontal=True
        )
        
        if "demo" in source_type.lower() and self.demo_widget:
            return self.demo_widget.render()
        else:
            df, metadata = self.file_widget.render()
            return df, metadata.filename if metadata else None

# ==============================
# G≈Å√ìWNA KLASA APLIKACJI UI
# ==============================

class TMIVApp:
    """G≈Ç√≥wna klasa aplikacji UI - tylko orchestracja komponent√≥w."""
    
    def __init__(self, data_config: DataConfig = None, ui_config: UIConfig = None):
        self.data_config = data_config or DataConfig()
        self.ui_config = ui_config or UIConfig()
        
        # Komponenty UI
        self.data_selector = DataSourceSelectorWidget(self.data_config, self.ui_config)
        self.target_selector = TargetSelectorWidget()
    
    def render_header(self) -> None:
        """Renderuje nag≈Ç√≥wek aplikacji."""
        st.title(self.ui_config.app_title)
        st.caption(self.ui_config.app_subtitle)
    
    def render_data_selection(self) -> Tuple[Optional[pd.DataFrame], Optional[str], Optional[str]]:
        """Renderuje sekcjƒô wyboru danych i targetu."""
        
        # Nag≈Ç√≥wek
        self.render_header()
        
        # Wyb√≥r danych
        df, dataset_name = self.data_selector.render()
        
        if df is None or df.empty:
            return None, None, None
        
        # Zapisz kolumny do session state dla kompatybilno≈õci
        st.session_state["df_columns"] = list(df.columns)
        
        # Wyb√≥r targetu
        target = self.target_selector.render(list(df.columns))
        
        return df, dataset_name, target
    
    def render_configuration(self, available_engines: List[str]) -> Dict[str, Any]:
        """Renderuje uproszczony panel konfiguracji."""
        
        with st.sidebar:
            st.header("‚öôÔ∏è Ustawienia")
            
            config = {}
            
            # Silnik ML
            config['ml_engine'] = st.selectbox(
                "Silnik ML",
                available_engines,
                index=0 if available_engines else 0,
                key="ml_engine"
            )
            
            # Podstawowe opcje
            config['cross_validation'] = st.checkbox("Cross-validation", value=True)
            config['feature_selection'] = st.checkbox("Selekcja cech", value=True)
            
            # Zaawansowane opcje
            with st.expander("üîß Zaawansowane"):
                config['hyperparameter_tuning'] = st.checkbox("Tuning hiperparametr√≥w", value=False)
                config['outlier_treatment'] = st.checkbox("Obs≈Çuga outlier√≥w", value=True)
        
        return config

class DataQualitySummaryWidget:
    """Widget do wy≈õwietlania podsumowania jako≈õci danych."""
    
    @staticmethod
    def render(df: pd.DataFrame) -> None:
        """Pokazuje podsumowanie jako≈õci danych."""
        
        if df.empty:
            return
        
        st.subheader("üìä Jako≈õƒá danych")
        
        # Podstawowe statystyki
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
            st.metric("Braki danych", f"{missing_pct:.1f}%")
        
        with col2:
            duplicates = df.duplicated().sum()
            st.metric("Duplikaty", duplicates)
        
        with col3:
            numeric_cols = len(df.select_dtypes(include=['number']).columns)
            st.metric("Kolumny numeryczne", numeric_cols)
        
        with col4:
            categorical_cols = len(df.select_dtypes(include=['object', 'category']).columns)
            st.metric("Kolumny kategoryczne", categorical_cols)
        
        # Szczeg√≥≈Çowa analiza brak√≥w
        missing_data = df.isnull().sum()
        missing_data = missing_data[missing_data > 0].sort_values(ascending=False)
        
        if not missing_data.empty:
            with st.expander(f"üîç Szczeg√≥≈Çy brak√≥w danych ({len(missing_data)} kolumn)", expanded=False):
                missing_df = pd.DataFrame({
                    'Kolumna': missing_data.index,
                    'Braki': missing_data.values,
                    'Procent': (missing_data.values / len(df) * 100).round(1)
                })
                st.dataframe(missing_df, use_container_width=True)

# ==============================
# FUNKCJE KOMPATYBILNO≈öCI WSTECZNEJ
# ==============================

def header() -> None:
    """Kompatybilno≈õƒá wsteczna - nag≈Ç√≥wek aplikacji."""
    config = UIConfig()
    st.title(config.app_title)
    st.caption(config.app_subtitle)

def dataset_selector(sample_data_path: Optional[Union[str, Path]] = None) -> Tuple[pd.DataFrame, str]:
    """Kompatybilno≈õƒá wsteczna - selector danych."""
    
    ui_config = UIConfig()
    if sample_data_path:
        ui_config.demo_data_paths = [Path(sample_data_path) / "avocado.csv"]
    
    data_config = DataConfig()
    selector = DataSourceSelectorWidget(data_config, ui_config)
    
    df, name = selector.render()
    return df or pd.DataFrame(), name or ""

def show_detected_target(auto_target: Optional[str], columns: Optional[List[str]] = None) -> Optional[str]:
    """Kompatybilno≈õƒá wsteczna - selector targetu."""
    
    cols = columns or st.session_state.get("df_columns", [])
    if not cols:
        return None
    
    target_selector = TargetSelectorWidget()
    return target_selector.render(cols)

def sidebar_config(
    available_ml: List[str],
    default_engine: str = "auto",
    show_eda_engine: bool = True,
) -> Dict[str, Any]:
    """Kompatybilno≈õƒá wsteczna - panel konfiguracji."""
    
    with st.sidebar:
        st.header("‚öôÔ∏è Ustawienia")
        
        config = {}
        
        if show_eda_engine:
            config["eda_engine"] = st.selectbox(
                "Silnik EDA",
                ["Szybkie podsumowanie", "Rozk≈Çady", "Korelacje", "Analiza zaawansowana"],
                key="eda_engine"
            )
        
        # Silnik ML
        default_idx = available_ml.index(default_engine) if default_engine in available_ml else 0
        config['ml_engine'] = st.selectbox(
            "Silnik ML",
            available_ml,
            index=default_idx,
            key="ml_engine"
        )
        
        # LLM (dla kompatybilno≈õci)
        config['llm_enabled'] = False
        config['llm_api_key'] = ""
        config['llm_prompt'] = ""
    
    return config

def upload_widget(
    config: Optional[DataConfig] = None,
    key: str = "main_uploader",
    label: str = "Wgraj plik danych"
) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
    """Prosty wrapper dla kompatybilno≈õci wstecznej."""
    
    upload_widget = FileUploadWidget(config or DataConfig())
    df, metadata = upload_widget.render(key)
    
    filename = metadata.filename if metadata else None
    return df, filename

def advanced_upload_widget(
    max_size_mb: int = 200,
    formats: Optional[List[str]] = None,
    enable_preview: bool = True,
    key: str = "advanced_uploader"
) -> Tuple[Optional[pd.DataFrame], Optional[FileMetadata]]:
    """Zaawansowany wrapper z customizacjƒÖ."""
    
    config = DataConfig(
        max_file_size_mb=max_size_mb,
        supported_formats=formats or ["csv", "json", "xlsx"],
        max_preview_rows=20 if enable_preview else 0
    )
    
    uploader = FileUploadWidget(config)
    return uploader.render(key)

def list_saved_runs(out_dir: Union[str, Path] = "tmiv_out") -> List[str]:
    """Kompatybilno≈õƒá wsteczna - lista zapisanych run√≥w."""
    base = Path(out_dir)
    if not base.exists():
        return []
    
    runs = [p.name for p in base.iterdir() if p.is_dir() and p.name.startswith("run_")]
    return sorted(runs)

# Funkcje pomocnicze zachowane dla kompatybilno≈õci
def _is_date_like_series(s: pd.Series) -> bool:
    """Sprawdza czy seria przypomina daty."""
    if pd.api.types.is_datetime64_any_dtype(s):
        return True
    if s.dtype == object:
        try:
            parsed = pd.to_datetime(s, errors="coerce", utc=False, infer_datetime_format=True)
            return parsed.notna().mean() >= 0.9
        except Exception:
            return False
    return False

def _auto_side_options(df: pd.DataFrame, target: Optional[str], task: Optional[str]) -> Dict[str, Any]:
    """Heurystyczne domy≈õlne opcje preprocessingu."""
    side = {
        "drop_constant": True,
        "auto_dates": True,
        "limit_cardinality": True,
        "high_card_topk": 50,
        "target_log1p": "auto",
        "target_winsor": "auto",
    }

    has_cat = any(
        (df[c].dtype == "object") or str(df[c].dtype).startswith("category")
        for c in df.columns if c != (target or "")
    )
    if not has_cat:
        side["limit_cardinality"] = False

    has_maybe_date = any(_is_date_like_series(df[c]) for c in df.columns if c != (target or ""))
    if not has_maybe_date:
        side["auto_dates"] = False

    if task == "clf":
        side["target_log1p"] = False
        side["target_winsor"] = False

    return side

# ==============================
# MAIN
# ==============================

if __name__ == "__main__":
    # Przyk≈Çad nowego API
    st.set_page_config(page_title="TMIV Demo", layout="wide")
    
    # Konfiguracja
    data_config = DataConfig(max_file_size_mb=500, max_preview_rows=50)
    ui_config = UIConfig(app_title="TMIV ‚Äî Enhanced Version", enable_llm=True)
    
    # Aplikacja
    app = TMIVApp(data_config, ui_config)
    df, dataset_name, target = app.render_data_selection()
    
    if df is not None and not df.empty:
        # Konfiguracja
        config = app.render_configuration(["auto", "xgboost", "lightgbm"])
        
        # Podsumowanie jako≈õci
        DataQualitySummaryWidget.render(df)
        
        # Poka≈º wybranƒÖ konfiguracjƒô
        st.subheader("‚öôÔ∏è Wybrana konfiguracja")
        st.json({
            "dataset": dataset_name,
            "target": target,
            "rows": len(df),
            "columns": len(df.columns),
            **config
        })
        
        st.success("‚úÖ Wszystko gotowe do analizy!")
    
    # Przyk≈Çad starego API (kompatybilno≈õƒá wsteczna)
    st.markdown("---")
    st.subheader("Kompatybilno≈õƒá wsteczna")
    
    # Stary spos√≥b u≈ºycia nadal dzia≈Ça
    if st.button("Test starego API"):
        header()
        old_df, old_name = dataset_selector("data")
        if not old_df.empty:
            target = show_detected_target(None, list(old_df.columns))
            config = sidebar_config(["auto", "xgboost"])
            st.write("Stare API dzia≈Ça!", {"dataset": old_name, "target": target})